{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konfiguration der Google Cloud und Herunterladen der JSON-Datei\n",
    "\n",
    "In diesem Notebook wird die Google Cloud konfiguriert und eine JSON-Datei heruntergeladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datei erfolgreich heruntergeladen zu /tmp/applicant_data_raw.json.\n"
     ]
    }
   ],
   "source": [
    "# Importieren der benötigten Bibliotheken\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Namen des Google Cloud Storage Buckets und der Quelldatei festlegen\n",
    "bucket_name = 'prod_prototype'\n",
    "source_blob_name = 'bronze/applicant_data_raw'\n",
    "local_temp_path = '/tmp/applicant_data_raw.json'\n",
    "\n",
    "# Dienstkonto-Datei laden\n",
    "service_account_json = '/Users/Kevin/Documents/GitHub/Transferarbeit/Prototyp_Transferarbeit_Lokal/Setup/prototyp-etl-pipline-d6cbb438aa70.json'\n",
    "credentials = service_account.Credentials.from_service_account_file(service_account_json)\n",
    "\n",
    "# Google Cloud Storage Client initialisieren\n",
    "client = storage.Client(credentials=credentials, project='prototyp-etl-pipline')\n",
    "bucket = client.bucket(bucket_name)\n",
    "blob = bucket.blob(source_blob_name)\n",
    "\n",
    "# Datei aus dem Bucket herunterladen\n",
    "blob.download_to_filename(local_temp_path)\n",
    "print(f\"Datei erfolgreich heruntergeladen zu {local_temp_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisiere Spark Session\n",
    "In diesem Notebook wird die Spark Session vorbereitet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/16 20:48:45 WARN Utils: Your hostname, MacBook-Pro-3.local resolves to a loopback address: 127.0.0.1; using 192.168.1.229 instead (on interface en0)\n",
      "24/06/16 20:48:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/Kevin/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/Kevin/.ivy2/jars\n",
      "com.google.cloud.bigdataoss#gcs-connector added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-17d94e45-0489-4a67-9e17-08fc388c898b;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/anaconda3/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.2 in central\n",
      "\tfound com.google.api-client#google-api-client-jackson2;1.31.3 in central\n",
      "\tfound com.google.api-client#google-api-client;1.31.3 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.31.2 in central\n",
      "\tfound com.google.http-client#google-http-client;1.39.0 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.14 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.google.guava#guava;30.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound org.checkerframework#checker-qual;3.5.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.5.1 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound io.opencensus#opencensus-api;0.28.0 in central\n",
      "\tfound io.grpc#grpc-context;1.37.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.28.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.39.0 in central\n",
      "\tfound com.google.code.gson#gson;2.8.6 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.39.0 in central\n",
      "\tfound com.google.cloud.bigdataoss#util;2.2.2 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.38.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.11.3 in central\n",
      "\tfound com.google.apis#google-api-services-iamcredentials;v1-rev20201022-1.31.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20201112-1.31.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.7.4 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;0.22.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;0.22.2 in central\n",
      "\tfound com.google.flogger#google-extensions;0.5.1 in central\n",
      "\tfound com.google.flogger#flogger;0.5.1 in central\n",
      "\tfound org.checkerframework#checker-compat-qual;2.5.3 in central\n",
      "\tfound com.google.flogger#flogger-system-backend;0.5.1 in central\n",
      "\tfound com.google.cloud.bigdataoss#util-hadoop;hadoop3-2.2.2 in central\n",
      "\tfound com.google.cloud.bigdataoss#gcsio;2.2.2 in central\n",
      "\tfound io.grpc#grpc-api;1.37.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.37.0 in central\n",
      "\tfound io.grpc#grpc-auth;1.37.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.37.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.37.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.14.0 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.0.1 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.37.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.37.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.1 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.37.0 in central\n",
      "\tfound io.grpc#grpc-core;1.37.0 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.0.5 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.14.0 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.19 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound io.perfmark#perfmark-api;0.23.0 in central\n",
      ":: resolution report :: resolve 530ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.11.3 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;1.31.3 from central in [default]\n",
      "\tcom.google.api-client#google-api-client-jackson2;1.31.3 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.0.1 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.0.5 from central in [default]\n",
      "\tcom.google.apis#google-api-services-iamcredentials;v1-rev20201022-1.31.0 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20201112-1.31.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;0.22.2 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;0.22.2 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.7.4 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.2 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#gcsio;2.2.2 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#util;2.2.2 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#util-hadoop;hadoop3-2.2.2 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.6 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.5.1 from central in [default]\n",
      "\tcom.google.flogger#flogger;0.5.1 from central in [default]\n",
      "\tcom.google.flogger#flogger-system-backend;0.5.1 from central in [default]\n",
      "\tcom.google.flogger#google-extensions;0.5.1 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;30.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.39.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.39.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.39.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.38.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.31.2 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.14.0 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.14.0 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.37.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.37.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.37.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.37.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.37.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.37.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.37.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.37.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.37.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.37.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.28.0 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.28.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.23.0 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.14 from central in [default]\n",
      "\torg.checkerframework#checker-compat-qual;2.5.3 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.5.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.19 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.http-client#google-http-client;1.38.0 by [com.google.http-client#google-http-client;1.39.0] in [default]\n",
      "\tcom.google.api-client#google-api-client;1.31.1 by [com.google.api-client#google-api-client;1.31.3] in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.4.0 by [com.google.errorprone#error_prone_annotations;2.5.1] in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.3.4 by [com.google.errorprone#error_prone_annotations;2.5.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   57  |   0   |   0   |   4   ||   53  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-17d94e45-0489-4a67-9e17-08fc388c898b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 53 already retrieved (0kB/8ms)\n",
      "24/06/16 20:48:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/16 20:48:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.229:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Datatransformation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1219a7590>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importieren der benötigten Bibliotheken\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Beispiel für das Hinzufügen des GCS Connectors zu einer lokalen Spark-Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Datatransformation\") \\\n",
    "    .config('spark.sql.debug.maxToStringFields', '1000') \\\n",
    "    .config(\"spark.jars.packages\", \"com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.2\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/Users/Kevin/Documents/GitHub/Transferarbeit/Prototyp_Transferarbeit_Lokal/Setup/prototyp-etl-pipline-d6cbb438aa70.json\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "# Überprüfen der SparkSession\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enttferung von Duplikaten\n",
    "In diesem Notebook werdne alle doppelten Daten entfernt, leere Zellen bleiben leer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der gelöschten Duplikate: 0\n",
      "Bereinigte Daten erfolgreich gespeichert zu /tmp/applicant_data_cleaned.parquet.\n"
     ]
    }
   ],
   "source": [
    "# JSON-Datei in ein Spark DataFrame laden\n",
    "df = spark.read.json('/tmp/applicant_data_raw.json')\n",
    "\n",
    "# Anzahl der ursprünglichen Datensätze\n",
    "initial_count = df.count()\n",
    "\n",
    "# Datenbereinigung: Entfernen von Duplikaten\n",
    "df_cleaned = df.dropDuplicates()\n",
    "\n",
    "# Anzahl der bereinigten Datensätze\n",
    "cleaned_count = df_cleaned.count()\n",
    "\n",
    "# Anzahl der gelöschten Duplikate\n",
    "duplicates_removed = initial_count - cleaned_count\n",
    "print(f\"Anzahl der gelöschten Duplikate: {duplicates_removed}\")\n",
    "\n",
    "# Bereinigte Daten in eine neue Parquet-Datei speichern\n",
    "cleaned_temp_path = '/tmp/applicant_data_cleaned.parquet'\n",
    "df_cleaned.write.mode('overwrite').parquet(cleaned_temp_path)\n",
    "print(f\"Bereinigte Daten erfolgreich gespeichert zu {cleaned_temp_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standartisierung Telefonnummern\n",
    "In diesem Notebook werden Telefonnummern vereinheitlich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ablehnungsgrund: string (nullable = true)\n",
      " |-- Adresse: string (nullable = true)\n",
      " |-- Bewerbungsdatum: string (nullable = true)\n",
      " |-- Bewerbungsquelle: string (nullable = true)\n",
      " |-- Bewertung Assessment: string (nullable = true)\n",
      " |-- Bewertung Erstes Gespräch: string (nullable = true)\n",
      " |-- Bewertung Prescreening: string (nullable = true)\n",
      " |-- E-Mail: string (nullable = true)\n",
      " |-- Effektiver Gehalt: double (nullable = true)\n",
      " |-- Einstellungsdatum: string (nullable = true)\n",
      " |-- Geburtsdatum: string (nullable = true)\n",
      " |-- Gehaltsvorstellungen: string (nullable = true)\n",
      " |-- Geschlecht: string (nullable = true)\n",
      " |-- Headhunter Firma: string (nullable = true)\n",
      " |-- Headhunter Name: string (nullable = true)\n",
      " |-- Interviewer: string (nullable = true)\n",
      " |-- Job Titel: string (nullable = true)\n",
      " |-- Kandidaten-ID: string (nullable = true)\n",
      " |-- Nachname: string (nullable = true)\n",
      " |-- Standort: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- StellenID: string (nullable = true)\n",
      " |-- Telefonnummer: string (nullable = true)\n",
      " |-- Veröffentlichungsdatum: string (nullable = true)\n",
      " |-- Vorname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      "\n",
      "Anzahl der durchgeführten Transformationen: 380\n",
      "Transformierte Daten erfolgreich gespeichert zu /tmp/applicant_data_transformed.parquet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/16 20:49:03 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# Importieren der benötigten Bibliotheken\n",
    "from pyspark.sql.functions import col, regexp_replace, length\n",
    "\n",
    "# Lokaler Pfad zur bereinigten Parquet-Datei\n",
    "cleaned_temp_path = '/tmp/applicant_data_cleaned.parquet'\n",
    "\n",
    "# Parquet-Datei in ein Spark DataFrame laden\n",
    "df_cleaned = spark.read.parquet(cleaned_temp_path)\n",
    "\n",
    "# Überprüfen der vorhandenen Spalten\n",
    "df_cleaned.printSchema()\n",
    "\n",
    "# Sicherstellen, dass die Spalte 'phone' existiert\n",
    "if 'Telefonnummer' not in df_cleaned.columns:\n",
    "    raise ValueError(\"Die Spalte 'phone' existiert nicht im DataFrame\")\n",
    "\n",
    "# Zählen der Telefonnummern vor der Transformation\n",
    "initial_phone_count = df_cleaned.filter(length(col(\"Telefonnummer\")) == 10).count()\n",
    "\n",
    "# Umwandeln der Telefonnummern in das gewünschte Format: +1-245-345-7426\n",
    "df_cleaned = df_cleaned.withColumn(\"phone\", \n",
    "                                   regexp_replace(\n",
    "                                       col(\"Telefonnummer\"), \n",
    "                                       r\"(\\d{1})(\\d{3})(\\d{3})(\\d{4})\", \n",
    "                                       r\"+1-$2-$3-$4\"\n",
    "                                   ))\n",
    "\n",
    "# Zählen der Telefonnummern nach der Transformation\n",
    "transformed_phone_count = df_cleaned.filter(col(\"Telefonnummer\").like(\"+1-%-%-%\")).count()\n",
    "\n",
    "# Anzahl der durchgeführten Transformationen\n",
    "transformations_done = transformed_phone_count\n",
    "print(f\"Anzahl der durchgeführten Transformationen: {transformations_done}\")\n",
    "\n",
    "# Bereinigte Daten in eine neue Parquet-Datei speichern\n",
    "transformed_temp_path = '/tmp/applicant_data_transformed.parquet'\n",
    "df_cleaned.write.mode('overwrite').parquet(transformed_temp_path)\n",
    "print(f\"Transformierte Daten erfolgreich gespeichert zu {transformed_temp_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auffüllen leerer Adresszellen\n",
    "\n",
    "In diesem Notebook werden leere Zellen in der Spalte Adresse mit dem Platzhalter \"Unknown\" gefüllt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren der benötigten Bibliotheken\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Lokaler Pfad zur bereinigten Parquet-Datei\n",
    "transformed_temp_path = '/tmp/applicant_data_transformed.parquet'\n",
    "\n",
    "# Parquet-Datei in ein Spark DataFrame laden\n",
    "df_cleaned = spark.read.parquet(transformed_temp_path)\n",
    "\n",
    "# Überprüfen der vorhandenen Spalten\n",
    "df_cleaned.printSchema()\n",
    "\n",
    "# Sicherstellen, dass die Spalte 'Adresse' existiert\n",
    "if 'Adresse' not in df_cleaned.columns:\n",
    "    raise ValueError(\"Die Spalte 'Adresse' existiert nicht im DataFrame\")\n",
    "\n",
    "# Zählen der leeren Adresszellen vor der Transformation\n",
    "initial_empty_address_count = df_cleaned.filter(col(\"Adresse\").isNull()).count()\n",
    "\n",
    "# Auffüllen leerer Zellen in der Spalte 'Adresse' mit 'Unknown'\n",
    "df_cleaned = df_cleaned.withColumn('Adresse', when(col('Adresse').isNull(), 'Unknown').otherwise(col('Adresse')))\n",
    "\n",
    "# Zählen der leeren Adresszellen nach der Transformation\n",
    "final_empty_address_count = df_cleaned.filter(col(\"Adresse\") == 'Unknown').count()\n",
    "\n",
    "# Anzahl der durchgeführten Auffüllungen\n",
    "fillings_done = final_empty_address_count\n",
    "print(f\"Anzahl der aufgefüllten Adresszellen: {fillings_done}\")\n",
    "\n",
    "# Bereinigte Daten in eine neue Parquet-Datei speichern\n",
    "final_temp_path = '/tmp/applicant_data_final.parquet'\n",
    "df_cleaned.write.mode('overwrite').parquet(final_temp_path)\n",
    "print(f\"Bereinigte Daten erfolgreich gespeichert zu {final_temp_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ablehnungsgrund: string (nullable = true)\n",
      " |-- Adresse: string (nullable = true)\n",
      " |-- Bewerbungsdatum: string (nullable = true)\n",
      " |-- Bewerbungsquelle: string (nullable = true)\n",
      " |-- Bewertung Assessment: string (nullable = true)\n",
      " |-- Bewertung Erstes Gespräch: string (nullable = true)\n",
      " |-- Bewertung Prescreening: string (nullable = true)\n",
      " |-- E-Mail: string (nullable = true)\n",
      " |-- Effektiver Gehalt: double (nullable = true)\n",
      " |-- Einstellungsdatum: string (nullable = true)\n",
      " |-- Geburtsdatum: string (nullable = true)\n",
      " |-- Gehaltsvorstellungen: string (nullable = true)\n",
      " |-- Geschlecht: string (nullable = true)\n",
      " |-- Headhunter Firma: string (nullable = true)\n",
      " |-- Headhunter Name: string (nullable = true)\n",
      " |-- Interviewer: string (nullable = true)\n",
      " |-- Job Titel: string (nullable = true)\n",
      " |-- Kandidaten-ID: string (nullable = true)\n",
      " |-- Nachname: string (nullable = true)\n",
      " |-- Standort: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- StellenID: string (nullable = true)\n",
      " |-- Telefonnummer: string (nullable = true)\n",
      " |-- Veröffentlichungsdatum: string (nullable = true)\n",
      " |-- Vorname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      "\n",
      "Anzahl der aufgefüllten Adresszellen: 127\n",
      "Bereinigte Daten erfolgreich gespeichert zu /tmp/applicant_data_final.parquet.\n"
     ]
    }
   ],
   "source": [
    "# Importieren der benötigten Bibliotheken\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Lokaler Pfad zur bereinigten Parquet-Datei\n",
    "transformed_temp_path = '/tmp/applicant_data_transformed.parquet'\n",
    "\n",
    "# Parquet-Datei in ein Spark DataFrame laden\n",
    "df_cleaned = spark.read.parquet(transformed_temp_path)\n",
    "\n",
    "# Überprüfen der vorhandenen Spalten\n",
    "df_cleaned.printSchema()\n",
    "\n",
    "# Sicherstellen, dass die Spalte 'Adresse' existiert\n",
    "if 'Adresse' not in df_cleaned.columns:\n",
    "    raise ValueError(\"Die Spalte 'Adresse' existiert nicht im DataFrame\")\n",
    "\n",
    "# Zählen der leeren Adresszellen vor der Transformation\n",
    "initial_empty_address_count = df_cleaned.filter(col(\"Adresse\").isNull()).count()\n",
    "\n",
    "# Auffüllen leerer Zellen in der Spalte 'Adresse' mit 'Unknown'\n",
    "df_cleaned = df_cleaned.withColumn('Adresse', when(col('Adresse').isNull(), 'Unknown').otherwise(col('Adresse')))\n",
    "\n",
    "# Zählen der leeren Adresszellen nach der Transformation\n",
    "final_empty_address_count = df_cleaned.filter(col(\"Adresse\") == 'Unknown').count()\n",
    "\n",
    "# Anzahl der durchgeführten Auffüllungen\n",
    "fillings_done = final_empty_address_count\n",
    "print(f\"Anzahl der aufgefüllten Adresszellen: {fillings_done}\")\n",
    "\n",
    "# Bereinigte Daten in eine neue Parquet-Datei speichern\n",
    "final_temp_path = '/tmp/applicant_data_final.parquet'\n",
    "df_cleaned.write.mode('overwrite').parquet(final_temp_path)\n",
    "print(f\"Bereinigte Daten erfolgreich gespeichert zu {final_temp_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hochladen der bereinigten Daten in den Google Cloud Storage\n",
    "\n",
    "In diesem Notebook werden die bereinigten Daten wieder in den Google Cloud Storage hochgeladen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /tmp/applicant_data_cleaned.parquet/part-00000-5a913d9c-6205-47e6-8ce9-db7c9e3dff06-c000.snappy.parquet uploaded to sensitive/datenbereinigung/part-00000-5a913d9c-6205-47e6-8ce9-db7c9e3dff06-c000.snappy.parquet.\n",
      "File /tmp/applicant_data_cleaned.parquet/._SUCCESS.crc uploaded to sensitive/datenbereinigung/._SUCCESS.crc.\n",
      "File /tmp/applicant_data_cleaned.parquet/_SUCCESS uploaded to sensitive/datenbereinigung/_SUCCESS.\n",
      "File /tmp/applicant_data_cleaned.parquet/.part-00000-5a913d9c-6205-47e6-8ce9-db7c9e3dff06-c000.snappy.parquet.crc uploaded to sensitive/datenbereinigung/.part-00000-5a913d9c-6205-47e6-8ce9-db7c9e3dff06-c000.snappy.parquet.crc.\n",
      "Bereinigte Dateien erfolgreich hochgeladen zu sensitive/datenbereinigung in bucket prod_prototype.\n"
     ]
    }
   ],
   "source": [
    "# Importieren der benötigten Bibliotheken\n",
    "import os\n",
    "\n",
    "# Funktion zum Hochladen eines Verzeichnisses zu Google Cloud Storage\n",
    "def upload_directory_to_gcs(bucket_name, source_directory, destination_blob_prefix):\n",
    "    client = storage.Client(credentials=credentials, project='prototyp-etl-pipline')\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    \n",
    "    for root, dirs, files in os.walk(source_directory):\n",
    "        for file in files:\n",
    "            local_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(local_path, source_directory)\n",
    "            blob_path = os.path.join(destination_blob_prefix, relative_path)\n",
    "            blob = bucket.blob(blob_path)\n",
    "            blob.upload_from_filename(local_path)\n",
    "            print(f\"File {local_path} uploaded to {blob_path}.\")\n",
    "\n",
    "# Namen des Google Cloud Storage Buckets und der bereinigten Datei festlegen\n",
    "bucket_name = 'prod_prototype'\n",
    "cleaned_blob_prefix = 'sensitive/datenbereinigung'\n",
    "cleaned_temp_path = '/tmp/applicant_data_cleaned.parquet'\n",
    "\n",
    "# Dienstkonto-Datei laden\n",
    "service_account_json = '/Users/Kevin/Documents/GitHub/Transferarbeit/Prototyp_Transferarbeit_Lokal/Setup/prototyp-etl-pipline-d6cbb438aa70.json'\n",
    "credentials = service_account.Credentials.from_service_account_file(service_account_json)\n",
    "\n",
    "# Hochladen des Parquet-Verzeichnisses\n",
    "upload_directory_to_gcs(bucket_name, cleaned_temp_path, cleaned_blob_prefix)\n",
    "print(f\"Bereinigte Dateien erfolgreich hochgeladen zu {cleaned_blob_prefix} in bucket {bucket_name}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
