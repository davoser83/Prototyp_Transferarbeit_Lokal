{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vorbereitung und Installation der erforderlichen Pakete\n",
    "\n",
    "In diesem Notebook werden die erforderlichen Pakete für PySpark und Google Cloud Storage installiert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/anaconda3/lib/python3.11/site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/anaconda3/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: google-cloud-storage in /opt/anaconda3/lib/python3.11/site-packages (2.17.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /opt/anaconda3/lib/python3.11/site-packages (from google-cloud-storage) (2.30.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-cloud-storage) (2.19.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-cloud-storage) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-cloud-storage) (2.7.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-cloud-storage) (2.31.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-cloud-storage) (1.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/anaconda3/lib/python3.11/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.63.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /opt/anaconda3/lib/python3.11/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (3.20.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/anaconda3/lib/python3.11/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.23.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (4.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2024.2.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.4.8)\n",
      "Requirement already satisfied: google-auth in /opt/anaconda3/lib/python3.11/site-packages (2.30.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "# Installiere die erforderlichen Pakete\n",
    "!pip install pyspark\n",
    "!pip install google-cloud-storage\n",
    "!pip install google-auth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisierung der SparkSession\n",
    "\n",
    "In diesem Notebook wird die SparkSession, die für die Verarbeitung mit PySpark benötigt wird installiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/16 08:42:20 WARN Utils: Your hostname, MacBook-Pro-3.local resolves to a loopback address: 127.0.0.1; using 192.168.1.229 instead (on interface en0)\n",
      "24/06/16 08:42:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/16 08:42:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.229:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ETL Pipeline Structure Creation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1125ff490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialisiere die SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL Pipeline Structure Creation\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Überprüfen der SparkSession\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Storage Konfiguration und Authentifizierung\n",
    "\n",
    "In diesem Notebook wird die Verbindung zu Google Cloud Storage und authentifiziert und mit einem Dienstkonto konfiguriert. Weiter wird eine Funktion, um Verzeichnisse in Google Cloud Storage zu erstellen definiert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.storage.client.Client object at 0x1250d8290>\n",
      "Created directory: test/\n"
     ]
    }
   ],
   "source": [
    "from google.oauth2 import service_account\n",
    "from google.cloud import storage\n",
    "\n",
    "# Dienstkonto-Datei laden\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    '/Users/Kevin/Documents/GitHub/Transferarbeit/Prototyp_Transferarbeit_Lokal/Setup/prototyp-etl-pipline-d6cbb438aa70.json')\n",
    "\n",
    "# Google Cloud Storage Client initialisieren\n",
    "client = storage.Client(credentials=credentials, project='prototyp-etl-pipline')\n",
    "\n",
    "# Überprüfen des Clients\n",
    "print(client)\n",
    "\n",
    "# Funktion zur Erstellung eines Verzeichnisses in GCS, wenn es nicht bereits existiert\n",
    "def create_directory(bucket_name, directory_name):\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(directory_name)\n",
    "    \n",
    "    if not blob.exists():  # Überprüfen, ob das Verzeichnis bereits existiert\n",
    "        blob.upload_from_string('')\n",
    "        print(f'Created directory: {directory_name}')\n",
    "    else:\n",
    "        print(f'Directory already exists: {directory_name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erstellung der Verzeichnisstruktur für die ETL-Pipeline\n",
    "\n",
    "In diesem Notebook wird die Verzeichnisstruktur für Bronze, Sensitive Data, Silver und Gold in Google Cloud Storage erstellt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: bronze/\n",
      "Created directory: sensitive/\n",
      "Created directory: silver/\n",
      "Created directory: gold/\n"
     ]
    }
   ],
   "source": [
    "# Pfade für die Verzeichnisse definieren\n",
    "bronze_path = f'gs://{bucket_name}/bronze/'\n",
    "sensitive_path = f'gs://{bucket_name}/sensitive/'\n",
    "silver_path = f'gs://{bucket_name}/silver/'\n",
    "gold_path = f'gs://{bucket_name}/gold/'\n",
    "\n",
    "# Erstelle die Verzeichnisse\n",
    "create_directory(bucket_name, 'bronze/')\n",
    "create_directory(bucket_name, 'sensitive/')\n",
    "create_directory(bucket_name, 'silver/')\n",
    "create_directory(bucket_name, 'gold/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schliessen der SparkSession\n",
    "\n",
    "In diesem Notebook wird die SparkSession, nachdem die Verzeichnisse erstellt wurde geschlossen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schliessen der SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
